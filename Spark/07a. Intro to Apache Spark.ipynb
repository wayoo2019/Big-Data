{"cells":[{"cell_type":"markdown","source":["### Introduction to Apache Spark\n\nIn this notebook we start to work with Apache Spark. This notebook is based on material supplied by Cloudera under their Cloudera Academic Partner program and the *Spark: The Definitive Guide* book by Bill Chambers and Matei Zaharia. You can find out more about Spark here: [https://spark.apache.org/](https://spark.apache.org/ \"Apache Spark\"). \n\nWe will use a Databricks Community Edition Spark Cluster for this notebook. Sign up for a free account here: [https://databricks.com/signup#signup/community](https://databricks.com/signup#signup/community)\n\nTopics\n- Working with text files\n- Working with delimited files\n- Working with Parquet files\n- Working with Hive tables\n- Generating a Spark Dataframe\n- Working with Pandas Dataframes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2871a8c-ebbf-41a3-ac8e-147ca977d50a"}}},{"cell_type":"code","source":["import urllib\n\n# Change the stuXXX number to match your own student ID\n# Get the ACCESS_KEY and SECRET_KEY from your stuXXX.txt file\n# MAKE THE NEEDED CHANGES TO THE NEXT THREE LINES  \nSTUDENT_NUMBER = \"stuXXX\"\nACCESS_KEY = \"XXXXXXXXXXXXXXXXXXXX\"\nSECRET_KEY = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n\n# Configure access to the course data on S3\n# You should only need to run this once\nENCODED_SECRET_KEY = urllib.parse.quote(SECRET_KEY, \"\")\nAWS_BUCKET_NAME = \"cis442f-course-data\"\nMOUNT_NAME = \"cis442f-data\"\ndbutils.fs.mount(\"s3n://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n\n# Configure access to your own location for storing data on S3\n# You should only need to run this once\n#AWS_BUCKET_NAME = \"cis442f-student-data/stu099\"\nAWS_BUCKET_NAME = \"cis442f-student-data/\"+STUDENT_NUMBER\nMOUNT_NAME = \"my-data\"\ndbutils.fs.mount(\"s3n://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fb14ab4-cf4a-4a28-88fe-3927bfc37eb8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[5]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# If you ever need to unmount these locations for any reason you can do so\n# by uncommenting and running the following two lines\n# dbutils.fs.unmount (\"/mnt/cis442f-data\")\n# dbutils.fs.unmount (\"/mnt/my-data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e32c04f-69fd-4580-ac70-af426aefbba5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["You  should have \n - Read access for `s3://cis442f-course-data` as `/mnt/cis442f-data` \n - Read/write access for `s3://cis442f-student-data/stuXXX` as `/mnt/my_data`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c363245-8784-4bdf-b839-c5ac926a10bb"}}},{"cell_type":"code","source":["# Check to see that you can see the contents of the S3 bucket\ndisplay(dbutils.fs.ls(\"/mnt/cis442f-data\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d46e0f7a-9312-4013-aa96-5c096b0f2ef5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/mnt/cis442f-data/duocar/","duocar/",0],["dbfs:/mnt/cis442f-data/for-hive/","for-hive/",0],["dbfs:/mnt/cis442f-data/input/","input/",0],["dbfs:/mnt/cis442f-data/output/","output/",0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/cis442f-data/duocar/</td><td>duocar/</td><td>0</td></tr><tr><td>dbfs:/mnt/cis442f-data/for-hive/</td><td>for-hive/</td><td>0</td></tr><tr><td>dbfs:/mnt/cis442f-data/input/</td><td>input/</td><td>0</td></tr><tr><td>dbfs:/mnt/cis442f-data/output/</td><td>output/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Check to see that you can see the contents of the S3 bucket where you can store data\ndisplay(dbutils.fs.ls(\"/mnt/my-data\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cfafdc5-8382-4dc5-a75c-40d33e462c9e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/mnt/my-data/backups/","backups/",0],["dbfs:/mnt/my-data/cluster-map.html","cluster-map.html",3450],["dbfs:/mnt/my-data/databricks-training/","databricks-training/",0],["dbfs:/mnt/my-data/duocar/","duocar/",0],["dbfs:/mnt/my-data/homework/","homework/",0],["dbfs:/mnt/my-data/map.html","map.html",2637],["dbfs:/mnt/my-data/myduocar/","myduocar/",0],["dbfs:/mnt/my-data/output/","output/",0],["dbfs:/mnt/my-data/practice/","practice/",0],["dbfs:/mnt/my-data/today/","today/",0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/my-data/backups/</td><td>backups/</td><td>0</td></tr><tr><td>dbfs:/mnt/my-data/cluster-map.html</td><td>cluster-map.html</td><td>3450</td></tr><tr><td>dbfs:/mnt/my-data/databricks-training/</td><td>databricks-training/</td><td>0</td></tr><tr><td>dbfs:/mnt/my-data/duocar/</td><td>duocar/</td><td>0</td></tr><tr><td>dbfs:/mnt/my-data/homework/</td><td>homework/</td><td>0</td></tr><tr><td>dbfs:/mnt/my-data/map.html</td><td>map.html</td><td>2637</td></tr><tr><td>dbfs:/mnt/my-data/myduocar/</td><td>myduocar/</td><td>0</td></tr><tr><td>dbfs:/mnt/my-data/output/</td><td>output/</td><td>0</td></tr><tr><td>dbfs:/mnt/my-data/practice/</td><td>practice/</td><td>0</td></tr><tr><td>dbfs:/mnt/my-data/today/</td><td>today/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# The databricks environment automatically creates a SparkSession for us\n# We can see it by just typing\nspark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42d36bf5-3931-462c-97da-02ed4d2f7d0b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[56]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[56]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=1954896675389977#setting/sparkui/0605-165546-terms523/driver-9045610561624467509\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=1954896675389977#setting/sparkui/0605-165546-terms523/driver-9045610561624467509\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["# The SparkSession has many properties including its own version\nspark.version"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b88b4e02-0f7f-49f9-8be4-1f1290cce6c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[57]: &#39;3.1.1&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[57]: &#39;3.1.1&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["A DataFrame is the most common Structured API. It represents a table of data with rows and columns as we have seen before. The list that defines the columns and the data types within the columns is called the _schema_. In this simple example we create a dataframe from individual data elements to illustrate several properties of DataFrames. More commonly the data will be imported from other sources."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2e65a48-70ba-4e50-bfd9-a216059e1dda"}}},{"cell_type":"code","source":["# Use the `createDataFrame` method to create a Spark DataFrame\nfrom pyspark.sql.types import *\n\nschema = StructType([StructField(\"class\", StringType()), StructField(\"student_id\", IntegerType())])\n\ndf = spark.createDataFrame([(\"Xiang\",1), (\"David\",2),(\"Jinghu\",3),(\"Sasha\",4),(\"Bin\",5), (\"Karthikeyan\",6),(\"Daniel\",7),(\"Luan\",8),(\"Yiqian\",9)], schema=schema)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08c8d11b-f65b-4574-9dc5-934268b393c3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Use `printSchema` method to print schema of the DataFrame\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"211c71fe-88cc-42f9-8c14-10683235f80a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- class: string (nullable = true)\n |-- student_id: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- class: string (nullable = true)\n-- student_id: integer (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Use the `show` method to view the DataFrame\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a424d5bf-aa00-4be3-b1f4-0342e1d2b679"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------+----------+\n|      class|student_id|\n+-----------+----------+\n|      Xiang|         1|\n|      David|         2|\n|     Jinghu|         3|\n|      Sasha|         4|\n|        Bin|         5|\n|Karthikeyan|         6|\n|     Daniel|         7|\n|       Luan|         8|\n|     Yiqian|         9|\n+-----------+----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+----------+\n      class|student_id|\n+-----------+----------+\n      Xiang|         1|\n      David|         2|\n     Jinghu|         3|\n      Sasha|         4|\n        Bin|         5|\nKarthikeyan|         6|\n     Daniel|         7|\n       Luan|         8|\n     Yiqian|         9|\n+-----------+----------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# A couple of helper functions to clean up directories if needed\n# They are used in this notebook to remove the outputs of previous\n# executions saved to S3 without throwing an error if they\n# do not exist (e.g. the first time the notebook is run)\n\ndef file_or_dir_exists(path):\n  try:\n    dbutils.fs.ls(path)\n    return True\n  except Exception as e:\n    if 'java.io.FileNotFoundException' in str(e):\n      return False\n    else:\n      raise\n      \ndef remove_dir_and_contents_if_exists (path):\n  if file_or_dir_exists (path):\n    dbutils.fs.rm(path, recurse=True)\n    print(path + \" removed\")\n  else:\n    print(path + \" did not exist\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bce7beec-adf8-4fc5-8951-b3b38530b0a4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### **Working with text files**\n\nAs you would expect there are many ways of loading and saving data. The `text` method of the [DataFrameReader](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader) class reads each line of of a text file into a row of a DataFrame with a single column named *value*.\n\nThis way of importing data would be suitable for capturing unstructured data e.g. html, xml or text for natural language processing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52a398d2-4f40-4be8-983d-503298e4fd96"}}},{"cell_type":"code","source":["# In this example we are reading data form S3\nproducts_txt = spark.read.text(\"/mnt/cis442f-data/input/examples8/products\")\n\nproducts_txt.show(5, truncate=False) # If 'truncate' set to True strings longer than 20 chars truncated. If set to a number, truncates long strings to that length. \nproducts_txt.head(5) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfe30e44-d458-41e5-ab62-040cf740026b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------------------------+\n|value                           |\n+--------------------------------+\n|1,Dualcore,USB Card Reader,18.39|\n|2,Dualcore,HDMI Cable,11.99     |\n|3,Dualcore,VGA Cable,1.99       |\n|4,Gigabux,6-cell Battery,40.50  |\n|5,Gigabux,8-cell Battery,50.50  |\n+--------------------------------+\nonly showing top 5 rows\n\nOut[62]: [Row(value=&#39;1,Dualcore,USB Card Reader,18.39&#39;),\n Row(value=&#39;2,Dualcore,HDMI Cable,11.99&#39;),\n Row(value=&#39;3,Dualcore,VGA Cable,1.99&#39;),\n Row(value=&#39;4,Gigabux,6-cell Battery,40.50&#39;),\n Row(value=&#39;5,Gigabux,8-cell Battery,50.50&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------+\nvalue                           |\n+--------------------------------+\n1,Dualcore,USB Card Reader,18.39|\n2,Dualcore,HDMI Cable,11.99     |\n3,Dualcore,VGA Cable,1.99       |\n4,Gigabux,6-cell Battery,40.50  |\n5,Gigabux,8-cell Battery,50.50  |\n+--------------------------------+\nonly showing top 5 rows\n\nOut[62]: [Row(value=&#39;1,Dualcore,USB Card Reader,18.39&#39;),\n Row(value=&#39;2,Dualcore,HDMI Cable,11.99&#39;),\n Row(value=&#39;3,Dualcore,VGA Cable,1.99&#39;),\n Row(value=&#39;4,Gigabux,6-cell Battery,40.50&#39;),\n Row(value=&#39;5,Gigabux,8-cell Battery,50.50&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The `text` method of the [DataFrameWriter](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter) of a text file for storing a dataframe (see below)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08670ac6-054f-4ef0-9d1f-4677c65ab738"}}},{"cell_type":"code","source":["# If the /practice/products_txt directory already exists the next cell would\n# report an error that \"path dbfs:/practice/products_txt already exists.\"\n\n# We can delete it if it already exists using the helper functions we created above\npath = \"/practice/products_txt\"\nremove_dir_and_contents_if_exists (path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17baf07d-de99-4120-abe9-9607ebf611ae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/practice/products_txt removed\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/practice/products_txt removed\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# We can store the data in text format to the cluster's storage (equivalant to the hdfs we have been working with)\nproducts_txt.write.text(\"/practice/products_txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a19e7d2-7df0-43da-afeb-782eda6f9301"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["You can find out more about the [Databricks file system](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html). This is how to list the contents of a directory"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdb4724c-04b5-4c45-a1e7-7c9eaaa52dae"}}},{"cell_type":"code","source":["# We can confirm that the data is there\ndisplay(dbutils.fs.ls(\"/practice/products_txt\"))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f78d6e5a-0418-4702-b759-f862baa72f97"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/practice/products_txt/_SUCCESS","_SUCCESS",0],["dbfs:/practice/products_txt/_committed_3014625248701639749","_committed_3014625248701639749",113],["dbfs:/practice/products_txt/_started_3014625248701639749","_started_3014625248701639749",0],["dbfs:/practice/products_txt/part-00000-tid-3014625248701639749-77ee3289-0e4c-4bcb-aabc-9766e1f3512f-174-1-c000.txt","part-00000-tid-3014625248701639749-77ee3289-0e4c-4bcb-aabc-9766e1f3512f-174-1-c000.txt",207]],"plotOptions":{"displayType":"html","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/practice/products_txt/_SUCCESS</td><td>_SUCCESS</td><td>0</td></tr><tr><td>dbfs:/practice/products_txt/_committed_3014625248701639749</td><td>_committed_3014625248701639749</td><td>113</td></tr><tr><td>dbfs:/practice/products_txt/_started_3014625248701639749</td><td>_started_3014625248701639749</td><td>0</td></tr><tr><td>dbfs:/practice/products_txt/part-00000-tid-3014625248701639749-77ee3289-0e4c-4bcb-aabc-9766e1f3512f-174-1-c000.txt</td><td>part-00000-tid-3014625248701639749-77ee3289-0e4c-4bcb-aabc-9766e1f3512f-174-1-c000.txt</td><td>207</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Extract the full path and file name of the first data file in the results directory\n\nimport re\n\nlisting = dbutils.fs.ls(\"/practice/products_txt\")\npath = \"/dbfs/practice/products_txt/\"\n\nfor item in listing:\n  if re.match(\".*part-.*\\\\.txt\", item[0]):\n    file_name = re.findall(\"part-.*\\\\.txt\", item[0])\n    break\n  \ndbfs_path_and_file = path + file_name[0]\ndbfs_path_and_file = dbfs_path_and_file[5:] # remove /dbfs prefix\n\nprint (dbfs_path_and_file)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd71b2f2-e9b5-4aa0-a9d7-5b20f540b8fb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/practice/products_txt/part-00000-tid-3014625248701639749-77ee3289-0e4c-4bcb-aabc-9766e1f3512f-174-1-c000.txt\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/practice/products_txt/part-00000-tid-3014625248701639749-77ee3289-0e4c-4bcb-aabc-9766e1f3512f-174-1-c000.txt\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Let's confirm that the data stored is what we expected\nprint(dbutils.fs.head(dbfs_path_and_file))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbe1c60d-f7eb-4404-854c-f840f76a8ff3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">1,Dualcore,USB Card Reader,18.39\n2,Dualcore,HDMI Cable,11.99\n3,Dualcore,VGA Cable,1.99\n4,Gigabux,6-cell Battery,40.50\n5,Gigabux,8-cell Battery,50.50\n6,Gigabux,Wall Charger,20.00\n7,Gigabux,Auto Charger,20.00\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">1,Dualcore,USB Card Reader,18.39\n2,Dualcore,HDMI Cable,11.99\n3,Dualcore,VGA Cable,1.99\n4,Gigabux,6-cell Battery,40.50\n5,Gigabux,8-cell Battery,50.50\n6,Gigabux,Wall Charger,20.00\n7,Gigabux,Auto Charger,20.00\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# We can delete this directory by uncommenting the following statement\n# Notice the similarity to Unix and hdfs commands\n# dbutils.fs.rm(\"/practice/products_txt\", recurse=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08810b3b-7e58-43f6-b56d-36c9c01f5a36"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["There is not much storage on the Databricks community cluster. So, we will use S3 instead for most of our work"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0979cfa3-2cb3-40be-b4cf-c70206001817"}}},{"cell_type":"markdown","source":["####**Working with S3**\n\nAs long as you ran the cell with your IAM key you should be able to read from and write to S3. \n- You have read access at `s3://cis442f-course-data` as `/mnt/cis442f-data`\n- You have read/write access at `s3://cis442f-student-data/stuXXX` as `/mnt/my_data`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a4742d0-4df9-401d-9fe8-ee6aa480bc44"}}},{"cell_type":"code","source":["# Reading the riders data set into a DataFrame using the `text` method of the DataFrameReader class\nriders_txt = spark.read.text(\"/mnt/cis442f-data/duocar/raw/riders/\")\nriders_txt.show(5, truncate=False)\nriders_txt.head(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95f8c81b-d28f-4b50-b476-ba58fa7eddba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------------------------------------------------------------------------------------------------------------------------+\n|value                                                                                                                      |\n+---------------------------------------------------------------------------------------------------------------------------+\n|id,birth_date,start_date,first_name,last_name,sex,ethnicity,student,home_block,home_lat,home_lon,work_lat,work_lon         |\n|220200000001,1962-03-18,2017-01-01,Natalie,Prosser,female,White,0,380170405002188,46.816399,-96.874038,46.831427,-96.827786|\n|220200000002,1981-10-06,2017-01-01,Nicholas,Murray,male,White,0,380170405002360,46.808599,-96.856890,,                     |\n|220200000003,1994-12-05,2017-01-01,Samuel,Zinanti,male,,0,380170103071039,46.821603,-96.806238,,                           |\n|220200000004,1970-05-31,2017-01-01,Kelcie,Flocken,female,White,0,380170102013021,46.873851,-96.906226,46.864072,-96.878836 |\n+---------------------------------------------------------------------------------------------------------------------------+\nonly showing top 5 rows\n\nOut[69]: [Row(value=&#39;id,birth_date,start_date,first_name,last_name,sex,ethnicity,student,home_block,home_lat,home_lon,work_lat,work_lon&#39;),\n Row(value=&#39;220200000001,1962-03-18,2017-01-01,Natalie,Prosser,female,White,0,380170405002188,46.816399,-96.874038,46.831427,-96.827786&#39;),\n Row(value=&#39;220200000002,1981-10-06,2017-01-01,Nicholas,Murray,male,White,0,380170405002360,46.808599,-96.856890,,&#39;),\n Row(value=&#39;220200000003,1994-12-05,2017-01-01,Samuel,Zinanti,male,,0,380170103071039,46.821603,-96.806238,,&#39;),\n Row(value=&#39;220200000004,1970-05-31,2017-01-01,Kelcie,Flocken,female,White,0,380170102013021,46.873851,-96.906226,46.864072,-96.878836&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------------------------------------------------------------------------------------------------------------------+\nvalue                                                                                                                      |\n+---------------------------------------------------------------------------------------------------------------------------+\nid,birth_date,start_date,first_name,last_name,sex,ethnicity,student,home_block,home_lat,home_lon,work_lat,work_lon         |\n220200000001,1962-03-18,2017-01-01,Natalie,Prosser,female,White,0,380170405002188,46.816399,-96.874038,46.831427,-96.827786|\n220200000002,1981-10-06,2017-01-01,Nicholas,Murray,male,White,0,380170405002360,46.808599,-96.856890,,                     |\n220200000003,1994-12-05,2017-01-01,Samuel,Zinanti,male,,0,380170103071039,46.821603,-96.806238,,                           |\n220200000004,1970-05-31,2017-01-01,Kelcie,Flocken,female,White,0,380170102013021,46.873851,-96.906226,46.864072,-96.878836 |\n+---------------------------------------------------------------------------------------------------------------------------+\nonly showing top 5 rows\n\nOut[69]: [Row(value=&#39;id,birth_date,start_date,first_name,last_name,sex,ethnicity,student,home_block,home_lat,home_lon,work_lat,work_lon&#39;),\n Row(value=&#39;220200000001,1962-03-18,2017-01-01,Natalie,Prosser,female,White,0,380170405002188,46.816399,-96.874038,46.831427,-96.827786&#39;),\n Row(value=&#39;220200000002,1981-10-06,2017-01-01,Nicholas,Murray,male,White,0,380170405002360,46.808599,-96.856890,,&#39;),\n Row(value=&#39;220200000003,1994-12-05,2017-01-01,Samuel,Zinanti,male,,0,380170103071039,46.821603,-96.806238,,&#39;),\n Row(value=&#39;220200000004,1970-05-31,2017-01-01,Kelcie,Flocken,female,White,0,380170102013021,46.873851,-96.906226,46.864072,-96.878836&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The `text` method can also write a compressed file. In this example to an S3 bucket. See [Compression Formats](https://www.cloudera.com/documentation/enterprise/latest/topics/introduction_compression.html) information from Cloudera.\n\nUse Cloudberry or Cyberduck to check that these directories have been written to your S3 bucket. Of course the rider data is actually in a csv format so we would want to read it as such rather than as simple text. We look at that next."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fa366d3-8601-44b8-aa7b-669643d3f16a"}}},{"cell_type":"code","source":["# If we ran the next cell before the output directories will exist\n# We can delete them if they exist\npath = \"/mnt/my-data/output/spark/riders_text_compressed/\"\nremove_dir_and_contents_if_exists (path)\n\npath = \"/mnt/my-data/output/spark/riders_text/\"\nremove_dir_and_contents_if_exists (path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e509c91-93e5-407b-a289-374da6f3a25e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/mnt/my-data/output/spark/riders_text_compressed/ removed\n/mnt/my-data/output/spark/riders_text/ removed\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/my-data/output/spark/riders_text_compressed/ removed\n/mnt/my-data/output/spark/riders_text/ removed\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Writing the riders dataframe to S3\nwrite_string = \"/mnt/my-data/output/spark/riders_text\"\nriders_txt.write.text(write_string)\n\n#Writing the riders dataframe to S3 in a compressed format\nwrite_string = \"/mnt/my-data/output/spark/riders_text_compressed\"\n# print(write_string)\nriders_txt.write.text(write_string, compression = \"bzip2\") "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aed0af59-6392-4c14-9201-0179b622ac11"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Check to see that you can see the contents of the S3 bucket where you can store data\ndisplay(dbutils.fs.ls(\"/mnt/my-data/output/spark\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"958d25ad-dee9-415e-b87d-d5d3cb093099"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/mnt/my-data/output/spark/riders_text/","riders_text/",0],["dbfs:/mnt/my-data/output/spark/riders_text_compressed/","riders_text_compressed/",0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/my-data/output/spark/riders_text/</td><td>riders_text/</td><td>0</td></tr><tr><td>dbfs:/mnt/my-data/output/spark/riders_text_compressed/</td><td>riders_text_compressed/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Use Cloudberry or Cyberduck to confirm that the compressed version really does take up less disk space"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b36c84f5-8c29-48df-abd5-2e30f9bec24f"}}},{"cell_type":"markdown","source":["####**Working with Delimited Data**\n\nThe rider data is actually a comma-delimited text file.  The `csv` method of `DataFrameReader` class reads a delimited text file.\n\nIn the following example we use the `csv` method and let Spark do its best to figure out the schema (`inferSchema`) from the data in each field"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5072900a-2894-40e4-8da2-bf8e2b0a0767"}}},{"cell_type":"code","source":["# Note that we did not have to create a new DataFrame to take a peek at what it contains. We asked Spark to just shows us a sample of the data as requested\n# Also note that backslashes (\\) are used to allow the command to span several lines. This can make code easier to read\nspark \\\n  .read \\\n  .csv(\"/mnt/cis442f-data/duocar/raw/riders/\", sep=\",\", header=True, inferSchema=True) \\\n  .show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efc7b75f-f929-4017-96cd-e174e4723400"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\n|          id|birth_date|start_date|first_name|last_name|   sex|ethnicity|student|     home_block| home_lat|  home_lon| work_lat|  work_lon|\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\n|220200000001|1962-03-18|2017-01-01|   Natalie|  Prosser|female|    White|      0|380170405002188|46.816399|-96.874038|46.831427|-96.827786|\n|220200000002|1981-10-06|2017-01-01|  Nicholas|   Murray|  male|    White|      0|380170405002360|46.808599| -96.85689|     null|      null|\n|220200000003|1994-12-05|2017-01-01|    Samuel|  Zinanti|  male|     null|      0|380170103071039|46.821603|-96.806238|     null|      null|\n|220200000004|1970-05-31|2017-01-01|    Kelcie|  Flocken|female|    White|      0|380170102013021|46.873851|-96.906226|46.864072|-96.878836|\n|220200000005|1962-12-12|2017-01-01|     Lucas|Devilbiss|  male|    White|      0|380170009032023|46.857253|-96.831356| 46.83171|-96.817859|\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\n          id|birth_date|start_date|first_name|last_name|   sex|ethnicity|student|     home_block| home_lat|  home_lon| work_lat|  work_lon|\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\n220200000001|1962-03-18|2017-01-01|   Natalie|  Prosser|female|    White|      0|380170405002188|46.816399|-96.874038|46.831427|-96.827786|\n220200000002|1981-10-06|2017-01-01|  Nicholas|   Murray|  male|    White|      0|380170405002360|46.808599| -96.85689|     null|      null|\n220200000003|1994-12-05|2017-01-01|    Samuel|  Zinanti|  male|     null|      0|380170103071039|46.821603|-96.806238|     null|      null|\n220200000004|1970-05-31|2017-01-01|    Kelcie|  Flocken|female|    White|      0|380170102013021|46.873851|-96.906226|46.864072|-96.878836|\n220200000005|1962-12-12|2017-01-01|     Lucas|Devilbiss|  male|    White|      0|380170009032023|46.857253|-96.831356| 46.83171|-96.817859|\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["This is actually a convenience function for the more general syntax in the next paragraph. \n\n**Note:** If you use either method with `header` set to `True`, Spark assumes that a header row occurs in *every* file in the data directory you load."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56c9befa-db6b-4836-874c-3bdc88060ccc"}}},{"cell_type":"code","source":["# Create a DataFrame from the raw data\nriders = spark \\\n  .read \\\n  .format(\"csv\") \\\n  .option(\"sep\", \",\") \\\n  .option(\"header\", True) \\\n  .option(\"inferSchema\", True) \\\n  .load(\"/mnt/cis442f-data/duocar/raw/riders/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e13fe26b-edf5-431b-8e8c-fd4248aabe6b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Spark does its best to infer the schema from the column names and values. \nThe `printSchema()` method shows us the schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59c46437-1c60-4ec5-afb5-f4b2fcc8a937"}}},{"cell_type":"code","source":["riders.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74e88658-ff40-49c2-a2cf-758c4059444c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- id: long (nullable = true)\n |-- birth_date: string (nullable = true)\n |-- start_date: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- sex: string (nullable = true)\n |-- ethnicity: string (nullable = true)\n |-- student: integer (nullable = true)\n |-- home_block: long (nullable = true)\n |-- home_lat: double (nullable = true)\n |-- home_lon: double (nullable = true)\n |-- work_lat: double (nullable = true)\n |-- work_lon: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- id: long (nullable = true)\n-- birth_date: string (nullable = true)\n-- start_date: string (nullable = true)\n-- first_name: string (nullable = true)\n-- last_name: string (nullable = true)\n-- sex: string (nullable = true)\n-- ethnicity: string (nullable = true)\n-- student: integer (nullable = true)\n-- home_block: long (nullable = true)\n-- home_lat: double (nullable = true)\n-- home_lon: double (nullable = true)\n-- work_lat: double (nullable = true)\n-- work_lon: double (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["You can manually specify the schema instead of inferring it from the header row and column value\n**Note** that the types specified are Spark Types imported from pyspark.sql.types. See Spark Documentation for more on [Spark Datatypes](http://spark.apache.org/docs/latest/sql-programming-guide.html#data-types)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d58d7651-463b-4fe8-9469-3949917ec817"}}},{"cell_type":"code","source":["from pyspark.sql.types import *\n# Specify the schema using this format of phython a StructType of python lists of field names and datattypes (using Spark Datatypes)\nschema = StructType([\n    StructField(\"id\", StringType()),\n    StructField(\"birth_date\", DateType()),\n    StructField(\"start_date\", DateType()),\n    StructField(\"first_name\", StringType()),\n    StructField(\"last_name\", StringType()),\n    StructField(\"sex\", StringType()),\n    StructField(\"ethnicity\", StringType()),\n    StructField(\"student\", IntegerType()),\n    StructField(\"home_block\", StringType()),\n    StructField(\"home_lat\", DoubleType()),\n    StructField(\"home_lon\", DoubleType()),\n    StructField(\"work_lat\", DoubleType()),\n    StructField(\"work_lon\", DoubleType())\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6959b881-eb70-4e18-9d73-0c3958ffa07f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Pass the schema to the `DataFrameReader`\nriders2 = spark \\\n  .read \\\n  .format(\"csv\") \\\n  .option(\"sep\", \",\") \\\n  .option(\"header\", True) \\\n  .schema(schema) \\\n  .load(\"/mnt/cis442f-data/duocar/raw/riders/\")\n\n# Note:We must include the header option otherwise Spark will read the\n# header row as a valid record\n\n# Confirm the explicit schema:\nriders2.printSchema()\n\n# Note that the Spark DataTypes have been mapped to python DataTypes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9216a0d-f8bc-44ad-b813-c7d17af3bd75"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- id: string (nullable = true)\n |-- birth_date: date (nullable = true)\n |-- start_date: date (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- sex: string (nullable = true)\n |-- ethnicity: string (nullable = true)\n |-- student: integer (nullable = true)\n |-- home_block: string (nullable = true)\n |-- home_lat: double (nullable = true)\n |-- home_lon: double (nullable = true)\n |-- work_lat: double (nullable = true)\n |-- work_lon: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- id: string (nullable = true)\n-- birth_date: date (nullable = true)\n-- start_date: date (nullable = true)\n-- first_name: string (nullable = true)\n-- last_name: string (nullable = true)\n-- sex: string (nullable = true)\n-- ethnicity: string (nullable = true)\n-- student: integer (nullable = true)\n-- home_block: string (nullable = true)\n-- home_lat: double (nullable = true)\n-- home_lon: double (nullable = true)\n-- work_lat: double (nullable = true)\n-- work_lon: double (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Remove existing tab delimited file if it exists\npath = \"/mnt/my-data/practice/riders_tsv\"\nremove_dir_and_contents_if_exists (path)\n\n# Write the file to a tab-delimited file:\nriders.write.csv(path, sep=\"\\t\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"288c2b5c-7759-4579-a0a1-ee79cd436ff9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/mnt/my-data/practice/riders_tsv removed\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/my-data/practice/riders_tsv removed\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Check to see that you can see the contents of the S3 bucket where you can store data\ndisplay(dbutils.fs.ls(\"/mnt/my-data/practice/riders_tsv\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5cede8d-d508-49a5-9dc3-ef9e965be926"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/mnt/my-data/practice/riders_tsv/_SUCCESS","_SUCCESS",0],["dbfs:/mnt/my-data/practice/riders_tsv/_committed_8156922563988798198","_committed_8156922563988798198",113],["dbfs:/mnt/my-data/practice/riders_tsv/_started_8156922563988798198","_started_8156922563988798198",0],["dbfs:/mnt/my-data/practice/riders_tsv/part-00000-tid-8156922563988798198-e5c62397-1e95-404e-ad82-0a3f19aea076-200-1-c000.csv","part-00000-tid-8156922563988798198-e5c62397-1e95-404e-ad82-0a3f19aea076-200-1-c000.csv",195207]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/my-data/practice/riders_tsv/_SUCCESS</td><td>_SUCCESS</td><td>0</td></tr><tr><td>dbfs:/mnt/my-data/practice/riders_tsv/_committed_8156922563988798198</td><td>_committed_8156922563988798198</td><td>113</td></tr><tr><td>dbfs:/mnt/my-data/practice/riders_tsv/_started_8156922563988798198</td><td>_started_8156922563988798198</td><td>0</td></tr><tr><td>dbfs:/mnt/my-data/practice/riders_tsv/part-00000-tid-8156922563988798198-e5c62397-1e95-404e-ad82-0a3f19aea076-200-1-c000.csv</td><td>part-00000-tid-8156922563988798198-e5c62397-1e95-404e-ad82-0a3f19aea076-200-1-c000.csv</td><td>195207</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Check that the data is saved the way you expect"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c380d1f1-3de7-4bf1-82f5-bdcf4796e083"}}},{"cell_type":"markdown","source":["###**Working with Parquet files**\n\n[Parquet](https://parquet.apache.org/) is a very popular columnar storage format for Hadoop.  Use the `parquet` method of the `DataFrameWriter` class to save a DataFrame in Parquet"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"932dd527-9eb5-4f06-89a0-ff0bbf76a948"}}},{"cell_type":"code","source":["#Remove existing parquet file if it exists\npath = \"/mnt/my-data/practice/riders_parquet/\"\nremove_dir_and_contents_if_exists (path)\n\nriders.write.parquet(path)\n\n# Note that the schema is stored with the data:\nspark.read.parquet(path).printSchema() "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00b5a9e7-1828-4fe3-b223-55b68bbb1bab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/mnt/my-data/practice/riders_parquet/ removed\nroot\n |-- id: long (nullable = true)\n |-- birth_date: string (nullable = true)\n |-- start_date: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- sex: string (nullable = true)\n |-- ethnicity: string (nullable = true)\n |-- student: integer (nullable = true)\n |-- home_block: long (nullable = true)\n |-- home_lat: double (nullable = true)\n |-- home_lon: double (nullable = true)\n |-- work_lat: double (nullable = true)\n |-- work_lon: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/my-data/practice/riders_parquet/ removed\nroot\n-- id: long (nullable = true)\n-- birth_date: string (nullable = true)\n-- start_date: string (nullable = true)\n-- first_name: string (nullable = true)\n-- last_name: string (nullable = true)\n-- sex: string (nullable = true)\n-- ethnicity: string (nullable = true)\n-- student: integer (nullable = true)\n-- home_block: long (nullable = true)\n-- home_lat: double (nullable = true)\n-- home_lon: double (nullable = true)\n-- work_lat: double (nullable = true)\n-- work_lon: double (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["####**Working with Hive tables in Spark**\nUse the `sql` method of the `SparkSession` class to run Hive queries (if the cluster has Hive installed)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"552517fc-36ab-4623-909a-031a4c2d6b02"}}},{"cell_type":"code","source":["# Use `sql` method of `SparkSession` class to run Hive queries\nspark.sql(\"SHOW DATABASES\").show()\n# spark.sql(\"USE examples\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb703fe2-a75a-4dba-8dbf-783baf400d47"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------------+\n|        databaseName|\n+--------------------+\n|david_tilson_simo...|\n|             default|\n|               dsfda|\n|              stu099|\n+--------------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\n        databaseName|\n+--------------------+\ndavid_tilson_simo...|\n             default|\n               dsfda|\n              stu099|\n+--------------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# We can create a table from an existing dataframe\nriders.createOrReplaceTempView(\"riders_data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22399f22-d222-4e7b-b517-d6bfca919dc6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Use `sql` method of `SparkSession` class to run Hive queries\nspark.sql(\"USE default\").show()\nspark.sql(\"SHOW TABLES\").show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29ae70d8-8e67-4fd6-b5ae-bc745d7b3534"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">++\n||\n++\n++\n\n+--------+--------------------+-----------+\n|database|           tableName|isTemporary|\n+--------+--------------------+-----------+\n| default|            avgtemps|      false|\n| default|           bikeshare|      false|\n| default|bikeshare_partiti...|      false|\n| default|        countrycodes|      false|\n| default|      databricksblog|      false|\n| default|           dcdataraw|      false|\n| default|          devicedata|      false|\n| default|            diamonds|      false|\n| default|           eventsraw|      false|\n| default|             flights|      false|\n| default|health_tracker_20...|      false|\n| default|health_tracker_da...|      false|\n| default|health_tracker_da...|      false|\n| default|health_tracker_da...|      false|\n| default|health_tracker_da...|      false|\n| default|health_tracker_da...|      false|\n| default| health_tracker_gold|      false|\n| default|health_tracker_si...|      false|\n| default|        movieratings|      false|\n| default|     outdoorproducts|      false|\n+--------+--------------------+-----------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">++\n|\n++\n++\n\n+--------+--------------------+-----------+\ndatabase|           tableName|isTemporary|\n+--------+--------------------+-----------+\n default|            avgtemps|      false|\n default|           bikeshare|      false|\n default|bikeshare_partiti...|      false|\n default|        countrycodes|      false|\n default|      databricksblog|      false|\n default|           dcdataraw|      false|\n default|          devicedata|      false|\n default|            diamonds|      false|\n default|           eventsraw|      false|\n default|             flights|      false|\n default|health_tracker_20...|      false|\n default|health_tracker_da...|      false|\n default|health_tracker_da...|      false|\n default|health_tracker_da...|      false|\n default|health_tracker_da...|      false|\n default|health_tracker_da...|      false|\n default| health_tracker_gold|      false|\n default|health_tracker_si...|      false|\n default|        movieratings|      false|\n default|     outdoorproducts|      false|\n+--------+--------------------+-----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Use `sql` method of `SparkSession` class to run Hive queries\nspark.sql(\"DESCRIBE riders_data\").show()\nspark.sql(\"SELECT * FROM riders_data LIMIT 10\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d95208bd-3017-4c05-a090-858d022c80b5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+---------+-------+\n|  col_name|data_type|comment|\n+----------+---------+-------+\n|        id|   bigint|   null|\n|birth_date|   string|   null|\n|start_date|   string|   null|\n|first_name|   string|   null|\n| last_name|   string|   null|\n|       sex|   string|   null|\n| ethnicity|   string|   null|\n|   student|      int|   null|\n|home_block|   bigint|   null|\n|  home_lat|   double|   null|\n|  home_lon|   double|   null|\n|  work_lat|   double|   null|\n|  work_lon|   double|   null|\n+----------+---------+-------+\n\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\n|          id|birth_date|start_date|first_name|last_name|   sex|ethnicity|student|     home_block| home_lat|  home_lon| work_lat|  work_lon|\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\n|220200000001|1962-03-18|2017-01-01|   Natalie|  Prosser|female|    White|      0|380170405002188|46.816399|-96.874038|46.831427|-96.827786|\n|220200000002|1981-10-06|2017-01-01|  Nicholas|   Murray|  male|    White|      0|380170405002360|46.808599| -96.85689|     null|      null|\n|220200000003|1994-12-05|2017-01-01|    Samuel|  Zinanti|  male|     null|      0|380170103071039|46.821603|-96.806238|     null|      null|\n|220200000004|1970-05-31|2017-01-01|    Kelcie|  Flocken|female|    White|      0|380170102013021|46.873851|-96.906226|46.864072|-96.878836|\n|220200000005|1962-12-12|2017-01-01|     Lucas|Devilbiss|  male|    White|      0|380170009032023|46.857253|-96.831356| 46.83171|-96.817859|\n|220200000006|1985-10-30|2017-01-01|    Skylar|  Peaslee|female|    White|      0|380170006001028|46.886456|-96.828385|     null|      null|\n|220200000008|1968-10-28|2017-01-01|    Darren|   Marble|  male|    White|      0|380170009014011|46.849767| -96.80089|     null|      null|\n|220200000009|1994-07-29|2017-01-01| Elizabeth|   Blanch|female|    White|      0|270270206001017|46.857531|-96.752339|     null|      null|\n|220200000010|1998-11-14|2017-01-01|     Jacob|   Savage|  male|    White|      0|380170103032030|46.829992|-96.828994|46.780244|-96.799106|\n|220200000011|1979-02-22|2017-01-01|    Cailyn|Broughton|female|    White|      0|380170401001317|46.992748|-97.610736|46.900602|-97.207745|\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+---------+-------+\n  col_name|data_type|comment|\n+----------+---------+-------+\n        id|   bigint|   null|\nbirth_date|   string|   null|\nstart_date|   string|   null|\nfirst_name|   string|   null|\n last_name|   string|   null|\n       sex|   string|   null|\n ethnicity|   string|   null|\n   student|      int|   null|\nhome_block|   bigint|   null|\n  home_lat|   double|   null|\n  home_lon|   double|   null|\n  work_lat|   double|   null|\n  work_lon|   double|   null|\n+----------+---------+-------+\n\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\n          id|birth_date|start_date|first_name|last_name|   sex|ethnicity|student|     home_block| home_lat|  home_lon| work_lat|  work_lon|\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\n220200000001|1962-03-18|2017-01-01|   Natalie|  Prosser|female|    White|      0|380170405002188|46.816399|-96.874038|46.831427|-96.827786|\n220200000002|1981-10-06|2017-01-01|  Nicholas|   Murray|  male|    White|      0|380170405002360|46.808599| -96.85689|     null|      null|\n220200000003|1994-12-05|2017-01-01|    Samuel|  Zinanti|  male|     null|      0|380170103071039|46.821603|-96.806238|     null|      null|\n220200000004|1970-05-31|2017-01-01|    Kelcie|  Flocken|female|    White|      0|380170102013021|46.873851|-96.906226|46.864072|-96.878836|\n220200000005|1962-12-12|2017-01-01|     Lucas|Devilbiss|  male|    White|      0|380170009032023|46.857253|-96.831356| 46.83171|-96.817859|\n220200000006|1985-10-30|2017-01-01|    Skylar|  Peaslee|female|    White|      0|380170006001028|46.886456|-96.828385|     null|      null|\n220200000008|1968-10-28|2017-01-01|    Darren|   Marble|  male|    White|      0|380170009014011|46.849767| -96.80089|     null|      null|\n220200000009|1994-07-29|2017-01-01| Elizabeth|   Blanch|female|    White|      0|270270206001017|46.857531|-96.752339|     null|      null|\n220200000010|1998-11-14|2017-01-01|     Jacob|   Savage|  male|    White|      0|380170103032030|46.829992|-96.828994|46.780244|-96.799106|\n220200000011|1979-02-22|2017-01-01|    Cailyn|Broughton|female|    White|      0|380170401001317|46.992748|-97.610736|46.900602|-97.207745|\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Note that the result of a Hive query is simply a Spark DataFrame. So, is is possible to use any of the many DataFrame methods that we will learn about on the results of a query from a table in Hive.\n It is also possible to save a DataFame as a Hive table and manipulate it using SQL type commands."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58c8fc53-b7eb-4bf1-b1aa-00b74643002b"}}},{"cell_type":"code","source":["riders_via_sql = spark.sql(\"SELECT * FROM riders_data\")\nriders_via_sql.printSchema()\nriders_via_sql.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50712455-8740-4478-9d27-18d9cd7f656a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- id: long (nullable = true)\n |-- birth_date: string (nullable = true)\n |-- start_date: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- sex: string (nullable = true)\n |-- ethnicity: string (nullable = true)\n |-- student: integer (nullable = true)\n |-- home_block: long (nullable = true)\n |-- home_lat: double (nullable = true)\n |-- home_lon: double (nullable = true)\n |-- work_lat: double (nullable = true)\n |-- work_lon: double (nullable = true)\n\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\n|          id|birth_date|start_date|first_name|last_name|   sex|ethnicity|student|     home_block| home_lat|  home_lon| work_lat|  work_lon|\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\n|220200000001|1962-03-18|2017-01-01|   Natalie|  Prosser|female|    White|      0|380170405002188|46.816399|-96.874038|46.831427|-96.827786|\n|220200000002|1981-10-06|2017-01-01|  Nicholas|   Murray|  male|    White|      0|380170405002360|46.808599| -96.85689|     null|      null|\n|220200000003|1994-12-05|2017-01-01|    Samuel|  Zinanti|  male|     null|      0|380170103071039|46.821603|-96.806238|     null|      null|\n|220200000004|1970-05-31|2017-01-01|    Kelcie|  Flocken|female|    White|      0|380170102013021|46.873851|-96.906226|46.864072|-96.878836|\n|220200000005|1962-12-12|2017-01-01|     Lucas|Devilbiss|  male|    White|      0|380170009032023|46.857253|-96.831356| 46.83171|-96.817859|\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- id: long (nullable = true)\n-- birth_date: string (nullable = true)\n-- start_date: string (nullable = true)\n-- first_name: string (nullable = true)\n-- last_name: string (nullable = true)\n-- sex: string (nullable = true)\n-- ethnicity: string (nullable = true)\n-- student: integer (nullable = true)\n-- home_block: long (nullable = true)\n-- home_lat: double (nullable = true)\n-- home_lon: double (nullable = true)\n-- work_lat: double (nullable = true)\n-- work_lon: double (nullable = true)\n\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\n          id|birth_date|start_date|first_name|last_name|   sex|ethnicity|student|     home_block| home_lat|  home_lon| work_lat|  work_lon|\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\n220200000001|1962-03-18|2017-01-01|   Natalie|  Prosser|female|    White|      0|380170405002188|46.816399|-96.874038|46.831427|-96.827786|\n220200000002|1981-10-06|2017-01-01|  Nicholas|   Murray|  male|    White|      0|380170405002360|46.808599| -96.85689|     null|      null|\n220200000003|1994-12-05|2017-01-01|    Samuel|  Zinanti|  male|     null|      0|380170103071039|46.821603|-96.806238|     null|      null|\n220200000004|1970-05-31|2017-01-01|    Kelcie|  Flocken|female|    White|      0|380170102013021|46.873851|-96.906226|46.864072|-96.878836|\n220200000005|1962-12-12|2017-01-01|     Lucas|Devilbiss|  male|    White|      0|380170009032023|46.857253|-96.831356| 46.83171|-96.817859|\n+------------+----------+----------+----------+---------+------+---------+-------+---------------+---------+----------+---------+----------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Create a database\nspark.sql(\"CREATE DATABASE IF NOT EXISTS stu099\").show()  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0541223b-e2c3-4eb3-85d7-a9fa57c2d14e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">++\n||\n++\n++\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">++\n|\n++\n++\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Create table name\ntable_name = \"stu099.riders_via_sql\"  \n\n# Use the `saveAsTable` method of the `DataFrameWriter`\n# class to save a DataFrame as a Hive table\nriders.write.saveAsTable(table_name)\n\n# You can now manipulate this table in Hive\nquery = \"DESCRIBE %s\" % table_name\nprint(query)\nspark.sql(query).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a95a4ac-a408-419b-9b23-2f23b48255b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">DESCRIBE stu099.riders_via_sql\n+---------------+---------+-------+\n|       col_name|data_type|comment|\n+---------------+---------+-------+\n|             id|   bigint|       |\n|     birth_date|   string|       |\n|     start_date|   string|       |\n|     first_name|   string|       |\n|      last_name|   string|       |\n|            sex|   string|       |\n|      ethnicity|   string|       |\n|        student|      int|       |\n|     home_block|   bigint|       |\n|       home_lat|   double|       |\n|       home_lon|   double|       |\n|       work_lat|   double|       |\n|       work_lon|   double|       |\n|               |         |       |\n| # Partitioning|         |       |\n|Not partitioned|         |       |\n+---------------+---------+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">DESCRIBE stu099.riders_via_sql\n+---------------+---------+-------+\n       col_name|data_type|comment|\n+---------------+---------+-------+\n             id|   bigint|       |\n     birth_date|   string|       |\n     start_date|   string|       |\n     first_name|   string|       |\n      last_name|   string|       |\n            sex|   string|       |\n      ethnicity|   string|       |\n        student|      int|       |\n     home_block|   bigint|       |\n       home_lat|   double|       |\n       home_lon|   double|       |\n       work_lat|   double|       |\n       work_lon|   double|       |\n               |         |       |\n # Partitioning|         |       |\nNot partitioned|         |       |\n+---------------+---------+-------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Now drop the table to clean up our database\nquery = \"DROP TABLE IF EXISTS %s\" % table_name\nprint(query)\nspark.sql(query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec3608a3-6fed-4ca5-8916-a1bf6922bcd5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">DROP TABLE IF EXISTS stu099.riders_via_sql\nOut[88]: DataFrame[]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">DROP TABLE IF EXISTS stu099.riders_via_sql\nOut[88]: DataFrame[]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["####**Generating a Spark DataFrame**\n\nSometimes we need to generate a Spark DataFrame from scratch, for example, for testing purposes.\nWe already saw the use of `createDataFrame` method to create a Spark DataFrame\n\n    from pyspark.sql.types import *\n    schema = StructType([StructField(\"class\", StringType()), StructField(\"student_id\", IntegerType())])\n    df = spark.createDataFrame([(\"Xiang\",1), (\"David\",2),(\"Jinghu\",3),(\"Sasha\",4),(\"Bin\",5), (\"Karthikeyan\",6),(\"Daniel\",7),(\"Luan\",8),(\"Yiqian\",9)], schema=schema)\n\nUse the `range` method to generate a sequence of integers and add new columns as appropriate."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cc56cb5-77af-4b61-a4db-647fe8690a48"}}},{"cell_type":"code","source":["# Use the `range` method to generate a sequence of integers and add new columns\n# as appropriate.\nspark.range(1000).show(5)\n\n# Use the `rand` function to generate a uniform random variable:\nfrom pyspark.sql.functions import rand\nspark \\\n  .range(1000) \\\n  .withColumn(\"uniform\", rand(12345)) \\\n  .show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d924ef74-3135-4064-aa05-67f27ecd5f54"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n|  3|\n|  4|\n+---+\nonly showing top 5 rows\n\n+---+--------------------+\n| id|             uniform|\n+---+--------------------+\n|  0| 0.35343661019324624|\n|  1|0.004888949820778...|\n|  2| 0.28441528031312124|\n|  3| 0.29827647928660106|\n|  4|  0.5804627817964109|\n+---+--------------------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+\n id|\n+---+\n  0|\n  1|\n  2|\n  3|\n  4|\n+---+\nonly showing top 5 rows\n\n+---+--------------------+\n id|             uniform|\n+---+--------------------+\n  0| 0.35343661019324624|\n  1|0.004888949820778...|\n  2| 0.28441528031312124|\n  3| 0.29827647928660106|\n  4|  0.5804627817964109|\n+---+--------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# or a Bernoulli random variable with `p = 0.25`:\nbern_df = spark \\\n  .range(1000) \\\n  .withColumn(\"Bernoulli\", (rand(12345) < 0.25).cast(\"int\"))\n  \n# Generate a summary using the functional style:\nbern_df.groupby(\"Bernoulli\").count().show()\n\n# Generate a summary using the SQL style:\nbern_df.createOrReplaceTempView(\"bern\")\nspark.sql(\"SELECT Bernoulli, COUNT(*) AS count \\\n    FROM bern \\\n    GROUP BY Bernoulli\") \\\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0dbf4165-834f-47c8-959d-c22123b83793"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------+-----+\n|Bernoulli|count|\n+---------+-----+\n|        1|  235|\n|        0|  765|\n+---------+-----+\n\n+---------+-----+\n|Bernoulli|count|\n+---------+-----+\n|        1|  235|\n|        0|  765|\n+---------+-----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+-----+\nBernoulli|count|\n+---------+-----+\n        1|  235|\n        0|  765|\n+---------+-----+\n\n+---------+-----+\nBernoulli|count|\n+---------+-----+\n        1|  235|\n        0|  765|\n+---------+-----+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Use the `randn` function to generate a normal random variable:\nfrom pyspark.sql.functions import randn\nran_df = spark.range(1000).withColumn(\"normal\", 42 +  2 * randn(54321))\nran_df.show(5)\n\n# In Spark we can use the `describe` method to get some overview statistics\n# Here the mean of the normal column is ~42 with a standard deviation ~2\nran_df.describe(\"id\", \"normal\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84162a23-8509-4998-93bd-307d6110be1f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---+------------------+\n| id|            normal|\n+---+------------------+\n|  0| 41.63113006505265|\n|  1| 41.13367775536738|\n|  2|40.997345190501456|\n|  3| 43.64993691222624|\n|  4|44.427341430261585|\n+---+------------------+\nonly showing top 5 rows\n\n+-------+-----------------+------------------+\n|summary|               id|            normal|\n+-------+-----------------+------------------+\n|  count|             1000|              1000|\n|   mean|            499.5| 41.97149716044106|\n| stddev|288.8194360957494|1.9982961836610456|\n|    min|                0| 36.07273156605738|\n|    max|              999|48.938949832297496|\n+-------+-----------------+------------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+------------------+\n id|            normal|\n+---+------------------+\n  0| 41.63113006505265|\n  1| 41.13367775536738|\n  2|40.997345190501456|\n  3| 43.64993691222624|\n  4|44.427341430261585|\n+---+------------------+\nonly showing top 5 rows\n\n+-------+-----------------+------------------+\nsummary|               id|            normal|\n+-------+-----------------+------------------+\n  count|             1000|              1000|\n   mean|            499.5| 41.97149716044106|\n stddev|288.8194360957494|1.9982961836610456|\n    min|                0| 36.07273156605738|\n    max|              999|48.938949832297496|\n+-------+-----------------+------------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Working with Pandas\nBe very careful not to download 'Big Data' into Pandas ... billions of rows could overwhelm your local machine. You would usually only download summary results or aggregations for further analysis or visualization"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6914e8e7-6fe0-4de4-8e9d-47fd45f4be04"}}},{"cell_type":"code","source":["#Note the size of the riders Spark Dataframe\nriders.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbf15015-ba24-4c1d-b2ed-bbcb5c2df29c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[92]: 1723</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[92]: 1723</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\n\n# take a sample of the riders Spark dataframe and load it into a Pandas dataframe\nriders_pd = riders.sample(0.05).toPandas()\nriders_pd.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f426beda-3a5d-4e7a-bfa2-f97aadc10187"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[93]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[93]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>birth_date</th>\n      <th>start_date</th>\n      <th>first_name</th>\n      <th>last_name</th>\n      <th>sex</th>\n      <th>ethnicity</th>\n      <th>student</th>\n      <th>home_block</th>\n      <th>home_lat</th>\n      <th>home_lon</th>\n      <th>work_lat</th>\n      <th>work_lon</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>220200000014</td>\n      <td>1998-07-08</td>\n      <td>2017-01-01</td>\n      <td>Robert</td>\n      <td>Dunnan</td>\n      <td>male</td>\n      <td>White</td>\n      <td>1</td>\n      <td>380170003002002</td>\n      <td>46.897359</td>\n      <td>-96.801023</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>220200000036</td>\n      <td>1996-12-15</td>\n      <td>2017-01-02</td>\n      <td>Ben</td>\n      <td>Sparks</td>\n      <td>male</td>\n      <td>White</td>\n      <td>1</td>\n      <td>380170003002004</td>\n      <td>46.895864</td>\n      <td>-96.805807</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>220200000037</td>\n      <td>1996-07-27</td>\n      <td>2017-01-02</td>\n      <td>Isaac</td>\n      <td>Schamel</td>\n      <td>male</td>\n      <td>White</td>\n      <td>0</td>\n      <td>380170103033007</td>\n      <td>46.822600</td>\n      <td>-96.826570</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>220200000041</td>\n      <td>1945-05-22</td>\n      <td>2017-01-02</td>\n      <td>Emily</td>\n      <td>Fredrickson</td>\n      <td>female</td>\n      <td>White</td>\n      <td>0</td>\n      <td>380170406003014</td>\n      <td>46.649363</td>\n      <td>-97.016428</td>\n      <td>46.645552</td>\n      <td>-97.008445</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>220200000042</td>\n      <td>1975-06-30</td>\n      <td>2017-01-02</td>\n      <td>Courtney</td>\n      <td>Tarpley</td>\n      <td>female</td>\n      <td>White</td>\n      <td>0</td>\n      <td>380170405003010</td>\n      <td>46.785502</td>\n      <td>-96.823170</td>\n      <td>46.840588</td>\n      <td>-96.868087</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>birth_date</th>\n      <th>start_date</th>\n      <th>first_name</th>\n      <th>last_name</th>\n      <th>sex</th>\n      <th>ethnicity</th>\n      <th>student</th>\n      <th>home_block</th>\n      <th>home_lat</th>\n      <th>home_lon</th>\n      <th>work_lat</th>\n      <th>work_lon</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>220200000014</td>\n      <td>1998-07-08</td>\n      <td>2017-01-01</td>\n      <td>Robert</td>\n      <td>Dunnan</td>\n      <td>male</td>\n      <td>White</td>\n      <td>1</td>\n      <td>380170003002002</td>\n      <td>46.897359</td>\n      <td>-96.801023</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>220200000036</td>\n      <td>1996-12-15</td>\n      <td>2017-01-02</td>\n      <td>Ben</td>\n      <td>Sparks</td>\n      <td>male</td>\n      <td>White</td>\n      <td>1</td>\n      <td>380170003002004</td>\n      <td>46.895864</td>\n      <td>-96.805807</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>220200000037</td>\n      <td>1996-07-27</td>\n      <td>2017-01-02</td>\n      <td>Isaac</td>\n      <td>Schamel</td>\n      <td>male</td>\n      <td>White</td>\n      <td>0</td>\n      <td>380170103033007</td>\n      <td>46.822600</td>\n      <td>-96.826570</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>220200000041</td>\n      <td>1945-05-22</td>\n      <td>2017-01-02</td>\n      <td>Emily</td>\n      <td>Fredrickson</td>\n      <td>female</td>\n      <td>White</td>\n      <td>0</td>\n      <td>380170406003014</td>\n      <td>46.649363</td>\n      <td>-97.016428</td>\n      <td>46.645552</td>\n      <td>-97.008445</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>220200000042</td>\n      <td>1975-06-30</td>\n      <td>2017-01-02</td>\n      <td>Courtney</td>\n      <td>Tarpley</td>\n      <td>female</td>\n      <td>White</td>\n      <td>0</td>\n      <td>380170405003010</td>\n      <td>46.785502</td>\n      <td>-96.823170</td>\n      <td>46.840588</td>\n      <td>-96.868087</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["demo_via_pandas = spark.createDataFrame(riders_pd)\ndemo_via_pandas.show(5)\ndemo_via_pandas.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d25b704e-9dfe-462f-afca-f21c32c1a910"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+----------+----------+----------+-----------+------+---------+-------+---------------+---------+----------+---------+----------+\n|          id|birth_date|start_date|first_name|  last_name|   sex|ethnicity|student|     home_block| home_lat|  home_lon| work_lat|  work_lon|\n+------------+----------+----------+----------+-----------+------+---------+-------+---------------+---------+----------+---------+----------+\n|220200000014|1998-07-08|2017-01-01|    Robert|     Dunnan|  male|    White|      1|380170003002002|46.897359|-96.801023|     null|      null|\n|220200000036|1996-12-15|2017-01-02|       Ben|     Sparks|  male|    White|      1|380170003002004|46.895864|-96.805807|     null|      null|\n|220200000037|1996-07-27|2017-01-02|     Isaac|    Schamel|  male|    White|      0|380170103033007|  46.8226| -96.82657|     null|      null|\n|220200000041|1945-05-22|2017-01-02|     Emily|Fredrickson|female|    White|      0|380170406003014|46.649363|-97.016428|46.645552|-97.008445|\n|220200000042|1975-06-30|2017-01-02|  Courtney|    Tarpley|female|    White|      0|380170405003010|46.785502| -96.82317|46.840588|-96.868087|\n+------------+----------+----------+----------+-----------+------+---------+-------+---------------+---------+----------+---------+----------+\nonly showing top 5 rows\n\nOut[94]: 83</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+----------+----------+----------+-----------+------+---------+-------+---------------+---------+----------+---------+----------+\n          id|birth_date|start_date|first_name|  last_name|   sex|ethnicity|student|     home_block| home_lat|  home_lon| work_lat|  work_lon|\n+------------+----------+----------+----------+-----------+------+---------+-------+---------------+---------+----------+---------+----------+\n220200000014|1998-07-08|2017-01-01|    Robert|     Dunnan|  male|    White|      1|380170003002002|46.897359|-96.801023|     null|      null|\n220200000036|1996-12-15|2017-01-02|       Ben|     Sparks|  male|    White|      1|380170003002004|46.895864|-96.805807|     null|      null|\n220200000037|1996-07-27|2017-01-02|     Isaac|    Schamel|  male|    White|      0|380170103033007|  46.8226| -96.82657|     null|      null|\n220200000041|1945-05-22|2017-01-02|     Emily|Fredrickson|female|    White|      0|380170406003014|46.649363|-97.016428|46.645552|-97.008445|\n220200000042|1975-06-30|2017-01-02|  Courtney|    Tarpley|female|    White|      0|380170405003010|46.785502| -96.82317|46.840588|-96.868087|\n+------------+----------+----------+----------+-----------+------+---------+-------+---------------+---------+----------+---------+----------+\nonly showing top 5 rows\n\nOut[94]: 83</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4b465a4-aa7d-43a4-8e56-5d04b776edb3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Hands On\n\n![Hands-on](https://cis442f-open-data.s3.amazonaws.com/pictures/hands.png \"Hands-on\")\n\n\n#### Exercises\n\n(1) Create a small dataframe called person with the following\n- data [(42, \"Tianyuan\", 11), (43, \"Ziran\", 11), (44, \"Yubo\", 12), (45, Ling, 16), (46, William, 17)] and\n- schema [\"id\", \"name\", \"city_id\"]\n\n(2) Use some methods we used above to gain insights about the DataFrame you just created\n- print the schema of the datafame\n- view its contents\n- count the number of records using the `count()` method\n    \n(3) Read the raw driver file from S3 into a Spark DataFrame.\n\n(4) Save the driver DataFrame as a JSON file in your practice directory.\n\n\nExpect to use the Databricks and/or Spark documentation as well as Google to figure out how to complete the following \n\n(5) Figure out how to inspect the JSON file.\n\n(6) Read the driver JSON file into a Spark DataFrame.\n\n(7) Figure out how to delete the JSON file from within this Databricks notebook \n\n(8) Figure out how to remove the practice subdirectory from your S3 storage from within this Databricks notebook\n\n\n**References you might need to learn about reading and writing in JSON data format**\n\n[DataFrameReader](http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.json.html#pyspark.sql.DataFrameReader.json)\n\n[DataFrameWriter](http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.json.html#pyspark.sql.DataFrameWriter.json)\n\n**The following are essential reference more generally**\n\n[PySpark Documentation](http://spark.apache.org/docs/latest/api/python/index.html)\n\n[PySpark API Reference ](http://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b19d4de9-d1a6-45ef-ae17-00f68309bd57"}}},{"cell_type":"markdown","source":["By the way . . . . Databricks has some datasets available for you to play with"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72b3bca1-4626-49d0-ba3a-67d7bada0dbb"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/databricks-datasets/\"))\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4641eb88-42b8-47da-bf07-129d8967545d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/databricks-datasets/","databricks-datasets/",0],["dbfs:/databricks-datasets/COVID/","COVID/",0],["dbfs:/databricks-datasets/README.md","README.md",976],["dbfs:/databricks-datasets/Rdatasets/","Rdatasets/",0],["dbfs:/databricks-datasets/SPARK_README.md","SPARK_README.md",3359],["dbfs:/databricks-datasets/adult/","adult/",0],["dbfs:/databricks-datasets/airlines/","airlines/",0],["dbfs:/databricks-datasets/amazon/","amazon/",0],["dbfs:/databricks-datasets/asa/","asa/",0],["dbfs:/databricks-datasets/atlas_higgs/","atlas_higgs/",0],["dbfs:/databricks-datasets/bikeSharing/","bikeSharing/",0],["dbfs:/databricks-datasets/cctvVideos/","cctvVideos/",0],["dbfs:/databricks-datasets/credit-card-fraud/","credit-card-fraud/",0],["dbfs:/databricks-datasets/cs100/","cs100/",0],["dbfs:/databricks-datasets/cs110x/","cs110x/",0],["dbfs:/databricks-datasets/cs190/","cs190/",0],["dbfs:/databricks-datasets/data.gov/","data.gov/",0],["dbfs:/databricks-datasets/definitive-guide/","definitive-guide/",0],["dbfs:/databricks-datasets/delta-sharing/","delta-sharing/",0],["dbfs:/databricks-datasets/flights/","flights/",0],["dbfs:/databricks-datasets/flower_photos/","flower_photos/",0],["dbfs:/databricks-datasets/flowers/","flowers/",0],["dbfs:/databricks-datasets/genomics/","genomics/",0],["dbfs:/databricks-datasets/hail/","hail/",0],["dbfs:/databricks-datasets/iot/","iot/",0],["dbfs:/databricks-datasets/iot-stream/","iot-stream/",0],["dbfs:/databricks-datasets/learning-spark/","learning-spark/",0],["dbfs:/databricks-datasets/learning-spark-v2/","learning-spark-v2/",0],["dbfs:/databricks-datasets/lending-club-loan-stats/","lending-club-loan-stats/",0],["dbfs:/databricks-datasets/med-images/","med-images/",0],["dbfs:/databricks-datasets/mnist-digits/","mnist-digits/",0],["dbfs:/databricks-datasets/news20.binary/","news20.binary/",0],["dbfs:/databricks-datasets/nyc-taxi-tiny/","nyc-taxi-tiny/",0],["dbfs:/databricks-datasets/nyctaxi/","nyctaxi/",0],["dbfs:/databricks-datasets/online_retail/","online_retail/",0],["dbfs:/databricks-datasets/overlap-join/","overlap-join/",0],["dbfs:/databricks-datasets/power-plant/","power-plant/",0],["dbfs:/databricks-datasets/retail-org/","retail-org/",0],["dbfs:/databricks-datasets/rwe/","rwe/",0],["dbfs:/databricks-datasets/sai-summit-2019-sf/","sai-summit-2019-sf/",0],["dbfs:/databricks-datasets/sample_logs/","sample_logs/",0],["dbfs:/databricks-datasets/samples/","samples/",0],["dbfs:/databricks-datasets/sfo_customer_survey/","sfo_customer_survey/",0],["dbfs:/databricks-datasets/sms_spam_collection/","sms_spam_collection/",0],["dbfs:/databricks-datasets/songs/","songs/",0],["dbfs:/databricks-datasets/structured-streaming/","structured-streaming/",0],["dbfs:/databricks-datasets/timeseries/","timeseries/",0],["dbfs:/databricks-datasets/tpch/","tpch/",0],["dbfs:/databricks-datasets/weather/","weather/",0],["dbfs:/databricks-datasets/wiki/","wiki/",0],["dbfs:/databricks-datasets/wikipedia-datasets/","wikipedia-datasets/",0],["dbfs:/databricks-datasets/wine-quality/","wine-quality/",0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/</td><td>databricks-datasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/COVID/</td><td>COVID/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/delta-sharing/</td><td>delta-sharing/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flower_photos/</td><td>flower_photos/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flowers/</td><td>flowers/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/med-images/</td><td>med-images/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyc-taxi-tiny/</td><td>nyc-taxi-tiny/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi/</td><td>nyctaxi/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/retail-org/</td><td>retail-org/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/rwe/</td><td>rwe/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/timeseries/</td><td>timeseries/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wine-quality/</td><td>wine-quality/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(dbutils.fs.head(\"/databricks-datasets/README.md\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfdc282f-df72-44e8-a2e0-175f8562d3c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Databricks Hosted Datasets\n==========================\n\nThe data contained within this directory is hosted for users to build \ndata pipelines using Apache Spark and Databricks.\n\nLicense\n-------\nUnless otherwise noted (e.g. within the README for a given data set), the data \nis licensed under Creative Commons Attribution 4.0 International (CC BY 4.0),\nwhich can be viewed at the following url:\n[http://creativecommons.org/licenses/by/4.0/legalcode](http://creativecommons.org/licenses/by/4.0/legalcode)\n\nContributions and Requests\n--------------------------\nTo request or contribute new datasets to this repository, please send an email\nto: hosted-datasets@databricks.com.\n\nWhen making the request, include the README.md file you want to publish. Make\nsure the file includes information about the source of the data, the license, \nand how to get additional information. Please ensure the license for this \ndata allows it to be hosted by Databricks and consumed by the public.\n\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Databricks Hosted Datasets\n==========================\n\nThe data contained within this directory is hosted for users to build \ndata pipelines using Apache Spark and Databricks.\n\nLicense\n-------\nUnless otherwise noted (e.g. within the README for a given data set), the data \nis licensed under Creative Commons Attribution 4.0 International (CC BY 4.0),\nwhich can be viewed at the following url:\n[http://creativecommons.org/licenses/by/4.0/legalcode](http://creativecommons.org/licenses/by/4.0/legalcode)\n\nContributions and Requests\n--------------------------\nTo request or contribute new datasets to this repository, please send an email\nto: hosted-datasets@databricks.com.\n\nWhen making the request, include the README.md file you want to publish. Make\nsure the file includes information about the source of the data, the license, \nand how to get additional information. Please ensure the license for this \ndata allows it to be hosted by Databricks and consumed by the public.\n\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(dbutils.fs.head(\"/databricks-datasets/README.md\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09972ae9-91dc-49c4-b81b-7af5dba4da50"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Databricks Hosted Datasets\n==========================\n\nThe data contained within this directory is hosted for users to build \ndata pipelines using Apache Spark and Databricks.\n\nLicense\n-------\nUnless otherwise noted (e.g. within the README for a given data set), the data \nis licensed under Creative Commons Attribution 4.0 International (CC BY 4.0),\nwhich can be viewed at the following url:\n[http://creativecommons.org/licenses/by/4.0/legalcode](http://creativecommons.org/licenses/by/4.0/legalcode)\n\nContributions and Requests\n--------------------------\nTo request or contribute new datasets to this repository, please send an email\nto: hosted-datasets@databricks.com.\n\nWhen making the request, include the README.md file you want to publish. Make\nsure the file includes information about the source of the data, the license, \nand how to get additional information. Please ensure the license for this \ndata allows it to be hosted by Databricks and consumed by the public.\n\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Databricks Hosted Datasets\n==========================\n\nThe data contained within this directory is hosted for users to build \ndata pipelines using Apache Spark and Databricks.\n\nLicense\n-------\nUnless otherwise noted (e.g. within the README for a given data set), the data \nis licensed under Creative Commons Attribution 4.0 International (CC BY 4.0),\nwhich can be viewed at the following url:\n[http://creativecommons.org/licenses/by/4.0/legalcode](http://creativecommons.org/licenses/by/4.0/legalcode)\n\nContributions and Requests\n--------------------------\nTo request or contribute new datasets to this repository, please send an email\nto: hosted-datasets@databricks.com.\n\nWhen making the request, include the README.md file you want to publish. Make\nsure the file includes information about the source of the data, the license, \nand how to get additional information. Please ensure the license for this \ndata allows it to be hosted by Databricks and consumed by the public.\n\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/databricks-datasets/COVID/CORD-19/CORD-19.readme.md\"))\nprint(dbutils.fs.head(\"/databricks-datasets/COVID/CORD-19/CORD-19.readme.md\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a1c541f-5b72-41e1-adf4-7009375fd0df"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/databricks-datasets/COVID/CORD-19/CORD-19.readme.md","CORD-19.readme.md",2592]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/COVID/CORD-19/CORD-19.readme.md</td><td>CORD-19.readme.md</td><td>2592</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">==========================================\r\nCOVID-19 Open Research Dataset\r\n==========================================\r\n\r\nThis data set was obtained from the Allen Institute or AI (https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).  Databricks is mirroring this dataset for use with\r\nthe Databricks Community Edition in the hope that it may be useful in making progress fighting COVID-19.\r\n\r\nThe data is provided by the Allen Insitute and is a compilation of data provided by the copyright holders and is subject to the licenses set forth in the all_sources_metadata_[Date] file contained with this readme.\r\n\t\t\r\n=========================================\r\nData Set Information\r\n=========================================\r\n\r\nIn response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource\r\nof over 29,000 scholarly articles, including over 13,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the \r\nglobal research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this \r\ninfectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research \r\ncommunity to keep up.\t\r\n\r\nDifferent versions of the data has been stored in their respective folders.\r\n\r\n=========================================\r\nLicense and/or Citation\r\n=========================================\r\n\r\nThis data set is licensed under multiple licenses, some of which require that derivate works be subject to the terms of the same license (e.g., CC-BY-NC-SA). See https://pages.semanticscholar.org/coronavirus-research for more information.\r\n\r\nCitations to the sources of the included files are included in the all_sources_metadata_[Date] file.\r\n\r\nPlease note that this dataset may only be used for NON-COMMERCIAL purposes consistent with the applicable licenses.\r\n\r\nDatabricks, Inc. DISCLAIMS ANY WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED.  Users of this dataset are solely responsible for their compliance with any copyright, patent or trademark\r\nrestrictions and are referred to the copyright, patent or trademark notices appearing in the original sources, all of which are hereby incorporated by reference.\r\n\r\nIf you believe this mirrored dataset infringes your rights, please contact legal@databricks.com\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">==========================================\r\nCOVID-19 Open Research Dataset\r\n==========================================\r\n\r\nThis data set was obtained from the Allen Institute or AI (https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).  Databricks is mirroring this dataset for use with\r\nthe Databricks Community Edition in the hope that it may be useful in making progress fighting COVID-19.\r\n\r\nThe data is provided by the Allen Insitute and is a compilation of data provided by the copyright holders and is subject to the licenses set forth in the all_sources_metadata_[Date] file contained with this readme.\r\n\t\t\r\n=========================================\r\nData Set Information\r\n=========================================\r\n\r\nIn response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource\r\nof over 29,000 scholarly articles, including over 13,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the \r\nglobal research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this \r\ninfectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research \r\ncommunity to keep up.\t\r\n\r\nDifferent versions of the data has been stored in their respective folders.\r\n\r\n=========================================\r\nLicense and/or Citation\r\n=========================================\r\n\r\nThis data set is licensed under multiple licenses, some of which require that derivate works be subject to the terms of the same license (e.g., CC-BY-NC-SA). See https://pages.semanticscholar.org/coronavirus-research for more information.\r\n\r\nCitations to the sources of the included files are included in the all_sources_metadata_[Date] file.\r\n\r\nPlease note that this dataset may only be used for NON-COMMERCIAL purposes consistent with the applicable licenses.\r\n\r\nDatabricks, Inc. DISCLAIMS ANY WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED.  Users of this dataset are solely responsible for their compliance with any copyright, patent or trademark\r\nrestrictions and are referred to the copyright, patent or trademark notices appearing in the original sources, all of which are hereby incorporated by reference.\r\n\r\nIf you believe this mirrored dataset infringes your rights, please contact legal@databricks.com\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Let's read in some flight data\nflightData2015 = spark.read.option(\"inferSchema\", \"true\") \\\n    .option(\"header\", \"true\").csv(\"/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f348c4b4-de04-484d-8882-ed883e6c54ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["flightData2015.show(5)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99735ac0-5413-46f7-92ad-e27a8b0bb758"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Romania|   15|\n    United States|            Croatia|    1|\n    United States|            Ireland|  344|\n            Egypt|      United States|   15|\n    United States|              India|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["flightData2015.sort(\"count\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b7e1768-44be-46f4-90f7-7ecd45ed74db"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|               Malta|      United States|    1|\n|Saint Vincent and...|      United States|    1|\n|       United States|            Croatia|    1|\n|       United States|          Gibraltar|    1|\n|       United States|          Singapore|    1|\n+--------------------+-------------------+-----+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+-------------------+-----+\n   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n               Malta|      United States|    1|\nSaint Vincent and...|      United States|    1|\n       United States|            Croatia|    1|\n       United States|          Gibraltar|    1|\n       United States|          Singapore|    1|\n+--------------------+-------------------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["flightData2015.sort(\"count\").explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d729beb3-fbb9-4752-9545-fb4fefc7d72a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Sort [count#3332 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(count#3332 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#2404]\n      +- FileScan csv [DEST_COUNTRY_NAME#3330,ORIGIN_COUNTRY_NAME#3331,count#3332] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Sort [count#3332 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(count#3332 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#2404]\n      +- FileScan csv [DEST_COUNTRY_NAME#3330,ORIGIN_COUNTRY_NAME#3331,count#3332] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sh\nls\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f679e903-db78-40d8-b8e0-af1c6a6d9dae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">conf\neventlogs\nganglia\nlogs\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">conf\neventlogs\nganglia\nlogs\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ecfaaeb-ad92-436f-bf71-8f6e987729e2"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"07a. Intro to Apache Spark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":112058074989677}},"nbformat":4,"nbformat_minor":0}
